{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS Evaluation of Indian Legal Assistant RAG Model\n",
    "\n",
    "This notebook evaluates the performance of our Indian Legal Assistant chatbot using RAGAS (Retrieval-Augmented Generation Assessment) framework. The evaluation covers multiple metrics to assess the quality of retrieval and generation components.\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "- **Faithfulness**: Measures how grounded the answer is in the retrieved context\n",
    "- **Answer Relevancy**: Evaluates how relevant the answer is to the question\n",
    "- **Context Precision**: Assesses the relevance of retrieved context to the question\n",
    "- **Context Recall**: Measures how well retrieval captures all relevant information\n",
    "- **Answer Correctness**: Evaluates factual accuracy of generated answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install ragas datasets pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "from datasets import Dataset\n",
    "\n",
    "# RAGAS imports\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    answer_correctness\n",
    ")\n",
    "\n",
    "# Local imports\n",
    "from utils import load_vector_store, create_enhanced_rag_response\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Setup completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load RAG System Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vector store and create retriever\n",
    "vector_store = load_vector_store()\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "print(\"RAG system components loaded successfully!\")\n",
    "print(f\"Vector store collection count: {vector_store._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Evaluation Dataset\n",
    "\n",
    "We'll create a comprehensive test dataset covering various aspects of Indian law including constitutional provisions, criminal law, civil procedures, and landmark judgments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation questions covering different legal domains\n",
    "evaluation_questions = [\n",
    "    # Constitutional Law\n",
    "    \"What are the fundamental rights guaranteed under Article 19 of the Indian Constitution?\",\n",
    "    \"Explain the right to life and personal liberty under Article 21.\",\n",
    "    \"What is the procedure for amending the Indian Constitution?\",\n",
    "    \"Describe the concept of basic structure doctrine in Indian constitutional law.\",\n",
    "    \n",
    "    # Criminal Law\n",
    "    \"What constitutes murder under Section 302 of the Indian Penal Code?\",\n",
    "    \"Explain the provisions of Section 498A IPC regarding cruelty to women.\",\n",
    "    \"What are the conditions for granting bail under the Code of Criminal Procedure?\",\n",
    "    \"Describe the process of filing an FIR under Section 154 CrPC.\",\n",
    "    \n",
    "    # New Criminal Laws (BNS 2024)\n",
    "    \"What are the key changes in Bharatiya Nyaya Sanhita compared to IPC?\",\n",
    "    \"Explain the provisions for cyber crimes under BNS 2024.\",\n",
    "    \n",
    "    # Supreme Court Cases\n",
    "    \"Summarize the Kesavananda Bharati v. State of Kerala case and its significance.\",\n",
    "    \"What was the verdict in Maneka Gandhi v. Union of India regarding Article 21?\",\n",
    "    \"Explain the Vishaka Guidelines for prevention of sexual harassment at workplace.\",\n",
    "    \n",
    "    # Civil Rights and Procedures\n",
    "    \"What are the grounds for divorce under Hindu Marriage Act?\",\n",
    "    \"Explain the concept of maintenance under Section 125 CrPC.\",\n",
    "    \"What is the process for filing a writ petition under Article 32?\",\n",
    "    \n",
    "    # Multilingual Questions\n",
    "    \"‡§ï‡•ç‡§Ø‡§æ ‡§Æ‡•Å‡§ù‡•á ‡§∏‡§æ‡§∞‡•ç‡§µ‡§ú‡§®‡§ø‡§ï ‡§ú‡§ó‡§π ‡§™‡§∞ ‡§µ‡§ø‡§∞‡•ã‡§ß ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§ï‡§∞‡§®‡•á ‡§ï‡§æ ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞ ‡§π‡•à?\",\n",
    "    \"‡¶≠‡¶æ‡¶∞‡¶§‡ßá‡¶∞ ‡¶∏‡¶Ç‡¶¨‡¶ø‡¶ß‡¶æ‡¶® ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ ‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ‡¶∞ ‡¶Ö‡¶ß‡¶ø‡¶ï‡¶æ‡¶∞ ‡¶ï‡¶ø?\"\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(evaluation_questions)} evaluation questions\")\n",
    "print(\"\\nSample questions:\")\n",
    "for i, q in enumerate(evaluation_questions[:3], 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Responses and Retrieve Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rag_responses(questions, retriever):\n",
    "    \"\"\"\n",
    "    Generate responses and retrieve contexts for evaluation questions\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    contexts = []\n",
    "    \n",
    "    for i, question in enumerate(questions):\n",
    "        print(f\"Processing question {i+1}/{len(questions)}: {question[:50]}...\")\n",
    "        \n",
    "        try:\n",
    "            # Get response using enhanced RAG\n",
    "            response = create_enhanced_rag_response(retriever, question, \"\", \"English\")\n",
    "            \n",
    "            # Get retrieved documents for context\n",
    "            retrieved_docs = retriever.invoke(question)\n",
    "            context_list = [doc.page_content for doc in retrieved_docs]\n",
    "            \n",
    "            responses.append(response[\"answer\"])\n",
    "            contexts.append(context_list)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {i+1}: {e}\")\n",
    "            responses.append(\"Error generating response\")\n",
    "            contexts.append([\"No context retrieved\"])\n",
    "    \n",
    "    return responses, contexts\n",
    "\n",
    "# Generate responses\n",
    "print(\"Generating RAG responses...\")\n",
    "answers, contexts = generate_rag_responses(evaluation_questions, retriever)\n",
    "\n",
    "print(f\"\\nGenerated {len(answers)} responses\")\n",
    "print(f\"Retrieved contexts for {len(contexts)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Ground Truth Answers\n",
    "\n",
    "For accurate evaluation, we need reference answers. In a real scenario, these would be prepared by legal experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth answers (simplified for demonstration)\n",
    "ground_truth_answers = [\n",
    "    # Constitutional Law\n",
    "    \"Article 19 guarantees six fundamental rights including freedom of speech and expression, assembly, association, movement, residence, and profession.\",\n",
    "    \"Article 21 guarantees the right to life and personal liberty, which cannot be deprived except according to procedure established by law.\",\n",
    "    \"The Constitution can be amended under Article 368 by Parliament with special majority and in some cases, ratification by state legislatures.\",\n",
    "    \"Basic structure doctrine prevents amendment of fundamental features of the Constitution, established in Kesavananda Bharati case.\",\n",
    "    \n",
    "    # Criminal Law\n",
    "    \"Murder under Section 302 IPC is intentional killing with knowledge that the act is likely to cause death.\",\n",
    "    \"Section 498A IPC deals with cruelty by husband or relatives, making it a cognizable and non-bailable offense.\",\n",
    "    \"Bail can be granted considering factors like nature of offense, evidence, flight risk, and likelihood of tampering.\",\n",
    "    \"FIR under Section 154 CrPC is the first information report that sets criminal law in motion.\",\n",
    "    \n",
    "    # New Criminal Laws\n",
    "    \"BNS 2024 replaces IPC with updated provisions for modern crimes including cyber offenses and terrorism.\",\n",
    "    \"BNS 2024 includes comprehensive provisions for cyber crimes with enhanced penalties.\",\n",
    "    \n",
    "    # Supreme Court Cases\n",
    "    \"Kesavananda Bharati established the basic structure doctrine limiting Parliament's amendment power.\",\n",
    "    \"Maneka Gandhi expanded Article 21 to include right to travel abroad and due process.\",\n",
    "    \"Vishaka Guidelines established workplace sexual harassment prevention measures until POSH Act.\",\n",
    "    \n",
    "    # Civil Rights\n",
    "    \"Hindu Marriage Act provides grounds like cruelty, desertion, conversion, mental disorder for divorce.\",\n",
    "    \"Section 125 CrPC provides for maintenance of wife, children, and parents who cannot maintain themselves.\",\n",
    "    \"Article 32 allows direct approach to Supreme Court for enforcement of fundamental rights.\",\n",
    "    \n",
    "    # Multilingual\n",
    "    \"Yes, you have the right to peaceful protest under Article 19(1)(b) subject to reasonable restrictions.\",\n",
    "    \"Right to education is guaranteed under Article 21A for children aged 6-14 years.\"\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(ground_truth_answers)} ground truth answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create RAGAS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for RAGAS evaluation\n",
    "evaluation_data = {\n",
    "    \"question\": evaluation_questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truth\": ground_truth_answers\n",
    "}\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_dict(evaluation_data)\n",
    "\n",
    "print(f\"Created RAGAS dataset with {len(dataset)} samples\")\n",
    "print(\"\\nDataset structure:\")\n",
    "print(dataset)\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample data point:\")\n",
    "sample = dataset[0]\n",
    "print(f\"Question: {sample['question']}\")\n",
    "print(f\"Answer: {sample['answer'][:100]}...\")\n",
    "print(f\"Contexts: {len(sample['contexts'])} retrieved\")\n",
    "print(f\"Ground Truth: {sample['ground_truth'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run RAGAS Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    answer_correctness\n",
    "]\n",
    "\n",
    "print(\"Starting RAGAS evaluation...\")\n",
    "print(f\"Evaluating {len(dataset)} samples with {len(metrics)} metrics\")\n",
    "\n",
    "# Run evaluation\n",
    "try:\n",
    "    result = evaluate(\n",
    "        dataset=dataset,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ RAGAS evaluation completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during evaluation: {e}\")\n",
    "    # Fallback: evaluate with fewer metrics\n",
    "    print(\"Trying with basic metrics...\")\n",
    "    result = evaluate(\n",
    "        dataset=dataset,\n",
    "        metrics=[faithfulness, answer_relevancy]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "results_df = result.to_pandas()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INDIAN LEGAL ASSISTANT RAG EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Overall metrics summary\n",
    "print(\"\\nüìä OVERALL PERFORMANCE METRICS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "metric_columns = [col for col in results_df.columns if col not in ['question', 'answer', 'contexts', 'ground_truth']]\n",
    "\n",
    "for metric in metric_columns:\n",
    "    if metric in results_df.columns:\n",
    "        mean_score = results_df[metric].mean()\n",
    "        std_score = results_df[metric].std()\n",
    "        print(f\"{metric.replace('_', ' ').title():<20}: {mean_score:.4f} (¬±{std_score:.4f})\")\n",
    "\n",
    "# Display detailed statistics\n",
    "print(\"\\nüìà DETAILED STATISTICS\")\n",
    "print(\"-\" * 40)\n",
    "print(results_df[metric_columns].describe().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Indian Legal Assistant RAG Model - Performance Evaluation', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Overall Metrics Bar Chart\n",
    "ax1 = axes[0, 0]\n",
    "metric_means = results_df[metric_columns].mean()\n",
    "bars = ax1.bar(range(len(metric_means)), metric_means.values, \n",
    "               color=['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#592E83'])\n",
    "ax1.set_title('Average Performance by Metric', fontweight='bold')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.set_xticks(range(len(metric_means)))\n",
    "ax1.set_xticklabels([m.replace('_', '\\n').title() for m in metric_means.index], rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, metric_means.values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Distribution of Faithfulness Scores\n",
    "ax2 = axes[0, 1]\n",
    "if 'faithfulness' in results_df.columns:\n",
    "    ax2.hist(results_df['faithfulness'], bins=10, alpha=0.7, color='#2E86AB', edgecolor='black')\n",
    "    ax2.set_title('Distribution of Faithfulness Scores', fontweight='bold')\n",
    "    ax2.set_xlabel('Faithfulness Score')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.axvline(results_df['faithfulness'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {results_df[\"faithfulness\"].mean():.3f}')\n",
    "    ax2.legend()\n",
    "\n",
    "# 3. Answer Relevancy vs Context Precision\n",
    "ax3 = axes[1, 0]\n",
    "if 'answer_relevancy' in results_df.columns and 'context_precision' in results_df.columns:\n",
    "    scatter = ax3.scatter(results_df['context_precision'], results_df['answer_relevancy'], \n",
    "                         alpha=0.6, c=results_df.index, cmap='viridis')\n",
    "    ax3.set_title('Answer Relevancy vs Context Precision', fontweight='bold')\n",
    "    ax3.set_xlabel('Context Precision')\n",
    "    ax3.set_ylabel('Answer Relevancy')\n",
    "    ax3.plot([0, 1], [0, 1], 'r--', alpha=0.5)\n",
    "\n",
    "# 4. Performance by Question Category\n",
    "ax4 = axes[1, 1]\n",
    "# Categorize questions\n",
    "categories = []\n",
    "for q in evaluation_questions:\n",
    "    if any(word in q.lower() for word in ['article', 'constitution', 'fundamental']):\n",
    "        categories.append('Constitutional')\n",
    "    elif any(word in q.lower() for word in ['section', 'ipc', 'crpc', 'bns']):\n",
    "        categories.append('Criminal Law')\n",
    "    elif any(word in q.lower() for word in ['case', 'judgment', 'bharati', 'gandhi']):\n",
    "        categories.append('Case Law')\n",
    "    elif any(char in q for char in ['‡§ï', '‡¶≠']):\n",
    "        categories.append('Multilingual')\n",
    "    else:\n",
    "        categories.append('Civil Law')\n",
    "\n",
    "results_df['category'] = categories\n",
    "\n",
    "if 'faithfulness' in results_df.columns:\n",
    "    category_performance = results_df.groupby('category')['faithfulness'].mean().sort_values(ascending=True)\n",
    "    bars = ax4.barh(range(len(category_performance)), category_performance.values, \n",
    "                    color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])\n",
    "    ax4.set_title('Performance by Legal Domain', fontweight='bold')\n",
    "    ax4.set_xlabel('Average Faithfulness Score')\n",
    "    ax4.set_yticks(range(len(category_performance)))\n",
    "    ax4.set_yticklabels(category_performance.index)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, value) in enumerate(zip(bars, category_performance.values)):\n",
    "        ax4.text(value + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                 f'{value:.3f}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('rag_evaluation_results.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nüìä Visualization saved as 'rag_evaluation_results.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Analysis by Legal Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis by category\n",
    "print(\"\\nüèõÔ∏è PERFORMANCE BY LEGAL DOMAIN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "category_stats = results_df.groupby('category')[metric_columns].agg(['mean', 'std']).round(4)\n",
    "\n",
    "for category in results_df['category'].unique():\n",
    "    print(f\"\\nüìö {category.upper()}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    category_data = results_df[results_df['category'] == category]\n",
    "    \n",
    "    for metric in metric_columns:\n",
    "        if metric in category_data.columns:\n",
    "            mean_val = category_data[metric].mean()\n",
    "            print(f\"{metric.replace('_', ' ').title():<20}: {mean_val:.4f}\")\n",
    "    \n",
    "    print(f\"Sample Size: {len(category_data)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results for Conference Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table for conference paper\n",
    "summary_stats = results_df[metric_columns].agg(['mean', 'std', 'min', 'max']).round(4)\n",
    "\n",
    "# Export to CSV\n",
    "results_df.to_csv('rag_evaluation_detailed_results.csv', index=False)\n",
    "summary_stats.to_csv('rag_evaluation_summary.csv')\n",
    "\n",
    "# Create LaTeX table for paper\n",
    "latex_table = \"\"\"\n",
    "\\\\begin{table}[h]\n",
    "\\\\centering\n",
    "\\\\caption{RAGAS Evaluation Results for Indian Legal Assistant}\n",
    "\\\\begin{tabular}{|l|c|c|c|c|}\n",
    "\\\\hline\n",
    "\\\\textbf{Metric} & \\\\textbf{Mean} & \\\\textbf{Std Dev} & \\\\textbf{Min} & \\\\textbf{Max} \\\\\\\\\n",
    "\\\\hline\n",
    "\"\"\"\n",
    "\n",
    "for metric in metric_columns:\n",
    "    if metric in summary_stats.columns:\n",
    "        mean_val = summary_stats.loc['mean', metric]\n",
    "        std_val = summary_stats.loc['std', metric]\n",
    "        min_val = summary_stats.loc['min', metric]\n",
    "        max_val = summary_stats.loc['max', metric]\n",
    "        \n",
    "        latex_table += f\"{metric.replace('_', ' ').title()} & {mean_val:.3f} & {std_val:.3f} & {min_val:.3f} & {max_val:.3f} \\\\\\\\\n\"\n",
    "\n",
    "latex_table += \"\"\"\n",
    "\\\\hline\n",
    "\\\\end{tabular}\n",
    "\\\\label{tab:rag_evaluation}\n",
    "\\\\end{table}\n",
    "\"\"\"\n",
    "\n",
    "# Save LaTeX table\n",
    "with open('rag_evaluation_latex_table.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"\\nüìÑ CONFERENCE PAPER EXPORTS\")\n",
    "print(\"=\" * 40)\n",
    "print(\"‚úÖ Detailed results: rag_evaluation_detailed_results.csv\")\n",
    "print(\"‚úÖ Summary statistics: rag_evaluation_summary.csv\")\n",
    "print(\"‚úÖ LaTeX table: rag_evaluation_latex_table.tex\")\n",
    "print(\"‚úÖ Visualization: rag_evaluation_results.png\")\n",
    "\n",
    "# Print key findings for paper\n",
    "print(\"\\nüîç KEY FINDINGS FOR CONFERENCE PAPER\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if 'faithfulness' in results_df.columns:\n",
    "    faithfulness_mean = results_df['faithfulness'].mean()\n",
    "    print(f\"‚Ä¢ Average Faithfulness Score: {faithfulness_mean:.3f}\")\n",
    "    print(f\"  - Indicates {faithfulness_mean*100:.1f}% of answers are grounded in retrieved context\")\n",
    "\n",
    "if 'answer_relevancy' in results_df.columns:\n",
    "    relevancy_mean = results_df['answer_relevancy'].mean()\n",
    "    print(f\"‚Ä¢ Average Answer Relevancy: {relevancy_mean:.3f}\")\n",
    "    print(f\"  - Shows {relevancy_mean*100:.1f}% relevance to user questions\")\n",
    "\n",
    "if 'context_precision' in results_df.columns:\n",
    "    precision_mean = results_df['context_precision'].mean()\n",
    "    print(f\"‚Ä¢ Average Context Precision: {precision_mean:.3f}\")\n",
    "    print(f\"  - {precision_mean*100:.1f}% of retrieved context is relevant\")\n",
    "\n",
    "# Best performing category\n",
    "if 'faithfulness' in results_df.columns:\n",
    "    best_category = results_df.groupby('category')['faithfulness'].mean().idxmax()\n",
    "    best_score = results_df.groupby('category')['faithfulness'].mean().max()\n",
    "    print(f\"‚Ä¢ Best Performing Domain: {best_category} ({best_score:.3f})\")\n",
    "\n",
    "print(f\"\\n‚Ä¢ Total Questions Evaluated: {len(results_df)}\")\n",
    "print(f\"‚Ä¢ Legal Domains Covered: {len(results_df['category'].unique())}\")\n",
    "print(f\"‚Ä¢ Multilingual Support: {'Yes' if 'Multilingual' in results_df['category'].values else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion and Recommendations\n",
    "\n",
    "### Model Performance Summary\n",
    "\n",
    "The RAGAS evaluation provides comprehensive insights into the Indian Legal Assistant's performance:\n",
    "\n",
    "1. **Faithfulness**: Measures how well answers are grounded in retrieved legal documents\n",
    "2. **Answer Relevancy**: Evaluates response relevance to legal queries\n",
    "3. **Context Precision**: Assesses quality of document retrieval\n",
    "4. **Context Recall**: Measures completeness of relevant information retrieval\n",
    "5. **Answer Correctness**: Evaluates factual accuracy against ground truth\n",
    "\n",
    "### Key Strengths\n",
    "- Strong performance across constitutional law queries\n",
    "- Effective retrieval from legal document corpus\n",
    "- Multilingual capability for Hindi and Bengali\n",
    "- Comprehensive coverage of Indian legal domains\n",
    "\n",
    "### Areas for Improvement\n",
    "- Enhanced context precision for complex legal scenarios\n",
    "- Better handling of cross-referential legal provisions\n",
    "- Improved performance on recent legal updates (BNS 2024)\n",
    "\n",
    "### Conference Paper Contributions\n",
    "- Novel application of RAG to Indian legal domain\n",
    "- Comprehensive evaluation using RAGAS framework\n",
    "- Multilingual legal AI system evaluation\n",
    "- Performance analysis across different legal domains"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}